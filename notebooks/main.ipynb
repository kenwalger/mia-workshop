{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Section 1: Introduction to Managed Inference & Agents (MIA) on Heroku ---\n",
    "\n",
    "## Welcome to the AIEngineer World's Fair Workshop!\n",
    "\n",
    "This interactive Jupyter Notebook will guide you through the exciting world of Managed Inference and Agents (MIA) on Heroku. We'll explore how Heroku simplifies deploying and interacting with AI models, leveraging powerful Heroku Tools, and understanding the foundational Model Context Protocol (MCP).\n",
    "\n",
    "**Workshop Agenda:**\n",
    "1.  **Introduction:** Setting the stage for AI and Agents on Heroku.\n",
    "2.  **Managed Inference:** Accessing and utilizing AI models through Heroku MIA.\n",
    "3.  **Heroku Tools:** Empowering agents to take action within your Heroku applications.\n",
    "4.  **Model Context Protocol (MCP):** The open standard enabling intelligent agent interactions.\n",
    "\n",
    "---\n",
    "\n",
    "## Setting up Your Environment (Pre-requisites)\n",
    "\n",
    "To run this notebook, you'll need:\n",
    "* A Heroku account.\n",
    "* A Heroku app with the Heroku Managed Inference and Agents (MIA) add-on attached.\n",
    "* Environment variables configured for your Heroku MIA endpoint and API key.\n",
    "\n",
    "```python\n",
    "# You would typically set these as Heroku Config Vars, but for local testing\n",
    "# or demonstration purposes, you might set them directly (for the workshop,\n",
    "# emphasize they should be config vars in a real app).\n",
    "\n",
    "# import os\n",
    "#\n",
    "# # Replace with your actual values (DO NOT COMMIT SENSITIVE KEYS)\n",
    "# os.environ[\"INFERENCE_URL\"] = \"YOUR_HEROKU_MIA_ENDPOINT_URL\"\n",
    "# %env INFERENCE_URL = \"YOUR_HEROKU_MIA_ENDPOINT_URL\"\n",
    "# os.environ[\"INFERENCE_KEY\"] = \"YOUR_HEROKU_MIA_API_KEY\"\n",
    "# os.environ[\"INFERENCE_MODEL_ID_CHAT\"] = \"claude-4-sonnet\" # Or other chat model\n",
    "# os.environ[\"INFERENCE_MODEL_ID_EMBED\"] = \"cohere-embed-multilingual\" # Or other embedding model\n",
    "# os.environ[\"TARGET_APP_NAME\"] = \"mia-workshop\" # The Heroku app your tools will interact with\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env INFERENCE_URL='YOUR_HEROKU_MIA_ENDPOINT_URL'\n",
    "%env INFERENCE_KEY='YOUR_HEROKU_MIA_API_KEY'\n",
    "%env INFERENCE_MODEL_ID_CHAT=claude-4-sonnet\n",
    "%env INFERENCE_MODEL_ID_EMBED=cohere-embed-mutilingual\n",
    "%env TARGET_APP_NAME=mia-workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Verify environment variables are set (for demonstration purposes)\n",
    "try:\n",
    "    INFERENCE_URL = os.environ.get(\"INFERENCE_URL\")\n",
    "    INFERENCE_KEY = os.environ.get(\"INFERENCE_KEY\")\n",
    "    INFERENCE_MODEL_ID_CHAT = os.environ.get(\"INFERENCE_MODEL_ID\")\n",
    "    INFERENCE_MODEL_ID_EMBED = os.environ.get(\"INFERENCE_MODEL_ID_EMBED\")\n",
    "    TARGET_APP_NAME = os.environ.get(\"TARGET_APP_NAME\") or \"mia-workshop\"\n",
    "    print(\"Environment variables loaded successfully!\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Missing environment variable {e}. Please ensure INFERENCE_URL, INFERENCE_KEY, INFERENCE_MODEL_ID_CHAT, INFERENCE_MODEL_ID_EMBED, and TARGET_APP_NAME are set.\")\n",
    "    # Exit or handle gracefully for workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Section 2: Managed Inference with Heroku MIA ---\n",
    "## What is Managed Inference?\n",
    "Heroku Managed Inference and Agents (MIA) simplifies the deployment and management of state-of-the-art AI models. Instead of handling complex infrastructure, you can access powerful foundational models as a managed service directly from your Heroku applications.\n",
    "\n",
    "This means:\n",
    "\n",
    "+ No server management: Heroku handles scaling, patching, and availability.\n",
    "+ Easy access: Interact with models via simple API calls.\n",
    "+ Focus on innovation: Concentrate on building intelligent features, not maintaining AI infrastructure.  \n",
    "\n",
    "Calling a Chat Model\n",
    "Let's start by making a simple API call to a chat model provisioned via Heroku MIA. We'll ask it to generate some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple Chat Completion\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {INFERENCE_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"model\": INFERENCE_MODEL_ID_CHAT,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Explain the concept of 'Managed Inference' in one sentence.\"}\n",
    "    ],\n",
    "    \"max_tokens\": 100\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(f\"{INFERENCE_URL}/v1/chat/completions\", headers=headers, json=payload)\n",
    "    response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "    result = response.json()\n",
    "\n",
    "    print(\"Model Response:\")\n",
    "    print(result['choices'][0]['message']['content'])\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error making API call: {e}\")\n",
    "    if response:\n",
    "        print(f\"Response status: {response.status_code}\")\n",
    "        print(f\"Response body: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Embedding Models (Optional - if time allows)  \n",
    "\n",
    "Embedding models convert text into numerical vector representations, which are crucial for tasks like semantic search, recommendation systems, and Retrieval Augmented Generation (RAG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Generating Embeddings (Conceptual)\n",
    "# This would require an embedding model provisioned in MIA\n",
    "\n",
    "# payload_embed = {\n",
    "#     \"model\": INFERENCE_MODEL_ID_EMBED,\n",
    "#     \"input\": [\"This is a sentence to embed.\", \"Another sentence.\"]\n",
    "# }\n",
    "#\n",
    "# try:\n",
    "#     response_embed = requests.post(f\"{INFERENCE_URL}/v1/embeddings\", headers=headers, json=payload_embed)\n",
    "#     response_embed.raise_for_status()\n",
    "#     embeddings = response_embed.json()\n",
    "#     print(\"\\nEmbeddings generated (first few dimensions of first embedding):\")\n",
    "#     print(embeddings['data'][0]['embedding'][:5]) # Print first 5 dimensions\n",
    "# except requests.exceptions.RequestException as e:\n",
    "#     print(f\"Error generating embeddings: {e}\")\n",
    "#     if response_embed:\n",
    "#         print(f\"Response status: {response_embed.status_code}\")\n",
    "#         print(f\"Response body: {response_embed.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Section 3: Heroku Tools: Enabling Agents to Act ---\n",
    "## The Power of Tools\n",
    "Large Language Models (LLMs) are incredible at understanding and generating human language, but they don't inherently know how to interact with external systems or perform actions. This is where Heroku Tools come in.\n",
    "\n",
    "Heroku Tools allow AI agents to:\n",
    "\n",
    "+ Execute commands: Run shell commands on your Heroku dynos.\n",
    "+ Query databases: Access and retrieve data from Heroku Postgres.\n",
    "+ Interact with APIs: Call custom APIs exposed by your Heroku apps.\n",
    "+ This capability transforms a static LLM into an intelligent agent that can perform actions within your application ecosystem.\n",
    "## Demo: Running a Command on a Heroku Dyno\n",
    "Let's demonstrate how an agent can use the dyno_run_command tool to get the current date and time from your Heroku application's dyno.\n",
    "\n",
    "_Pre-requisite_: Your Heroku app (TARGET_APP_NAME) needs to have the MIA add-on attached and the dyno_run_command tool configured (this is usually done via Heroku CLI or a tool definition in your agent setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Agent using dyno_run_command to get current time\n",
    "\n",
    "payload_tool = {\n",
    "    \"model\": INFERENCE_MODEL_ID_CHAT,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is the current date and time on the server?\"}\n",
    "    ],\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"type\": \"heroku_tool\",\n",
    "            \"name\": \"dyno_run_command\",\n",
    "            \"runtime_params\": {\n",
    "                \"target_app_name\": TARGET_APP_NAME,\n",
    "                \"tool_params\": {\n",
    "                    \"cmd\": \"date\",\n",
    "                    \"description\": \"Runs the 'date' command on a one-off dyno to get the current date and time.\",\n",
    "                    \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    print(f\"Attempting to call Heroku MIA agent endpoint for app: {TARGET_APP_NAME}...\")\n",
    "    response_tool = requests.post(f\"{INFERENCE_URL}/v1/agents/heroku\", headers=headers, json=payload_tool)\n",
    "    response_tool.raise_for_status()\n",
    "    result_tool = response_tool.json()\n",
    "\n",
    "    print(\"\\nAgent Response (including tool call information):\")\n",
    "    # This structure can vary slightly based on the LLM's tool calling output\n",
    "    if 'choices' in result_tool and result_tool['choices']:\n",
    "        message = result_tool['choices'][0]['message']\n",
    "        if 'tool_calls' in message:\n",
    "            print(\"Agent decided to call a tool:\")\n",
    "            for tool_call in message['tool_calls']:\n",
    "                print(f\"  Tool Name: {tool_call['function']['name']}\")\n",
    "                print(f\"  Tool Arguments: {tool_call['function']['arguments']}\")\n",
    "        if 'content' in message:\n",
    "            print(\"  Agent's final response:\")\n",
    "            print(message['content'])\n",
    "    else:\n",
    "        print(\"No choices found in response or unexpected format.\")\n",
    "        print(result_tool) # Print raw result for debugging\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error making API call with tool: {e}\")\n",
    "    if response_tool:\n",
    "        print(f\"Response status: {response_tool.status_code}\")\n",
    "        print(f\"Response body: {response_tool.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Querying a Heroku Postgres Database (Conceptual)\n",
    "This would involve a similar structure to the dyno_run_command example, but using the postgres_run_query tool. You'd need a Heroku Postgres add-on attached and a follower database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Agent using postgres_run_query (Conceptual)\n",
    "\n",
    "# payload_db_query = {\n",
    "#     \"model\": INFERENCE_MODEL_ID_CHAT,\n",
    "#     \"messages\": [\n",
    "#         {\"role\": \"user\", \"content\": \"How many users are in my 'users' table?\"}\n",
    "#     ],\n",
    "#     \"tools\": [\n",
    "#         {\n",
    "#             \"type\": \"heroku_tool\",\n",
    "#             \"name\": \"postgres_run_query\",\n",
    "#             \"runtime_params\": {\n",
    "#                 \"target_app_name\": TARGET_APP_NAME,\n",
    "#                 \"tool_params\": {\n",
    "#                     \"db_attachment\": os.environ.get(\"DATABASE_URL\"), # Or specific alias\n",
    "#                     \"query\": \"SELECT COUNT(*) FROM users;\",\n",
    "#                     \"description\": \"Runs a SQL query on the 'users' table.\",\n",
    "#                     \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}\n",
    "#                 }\n",
    "#             }\n",
    "#         }\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# ... (similar requests.post and result parsing logic as above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Section 4: Model Context Protocol (MCP) ---\n",
    "## What is MCP? The \"USB-C for AI\"\n",
    "The Model Context Protocol (MCP) is an open standard, initially introduced by Anthropic, that provides a universal and standardized way for AI models (especially agents) to interact with external data sources, tools, and environments.\n",
    "\n",
    "Think of it like a USB-C port for AI applications. Instead of requiring bespoke integrations for every tool or data source, MCP defines a common \"language\" (built on JSON-RPC 2.0) that enables seamless communication.\n",
    "\n",
    "## Why MCP Matters for Heroku MIA\n",
    "Heroku's Managed Inference and Agents leverages MCP to provide its powerful agentic capabilities.\n",
    "\n",
    "+ Standardization: Ensures consistent interaction between AI agents and your applications/tools.\n",
    "+ Security: Defines a secure framework for context exchange.\n",
    "+ Scalability: Designed to handle complex interactions across diverse systems.\n",
    "+ Interoperability: Promotes a richer ecosystem where AI agents can easily integrate with various services.\n",
    "\n",
    "## How it Works (High-Level Concept)\n",
    "+ Host Process (Heroku MIA): Manages the lifecycle and security policies for AI agents.\n",
    "+ Client Instances (AI Model/Agent): Runs within the host, negotiating capabilities and orchestrating messages.\n",
    "+ MCP Server (Your Heroku App/Tool): Exposes functionalities (like dyno_run_command or postgres_run_query) via MCP.\n",
    "When an AI agent needs to perform an action (like running a command), it sends an MCP-compliant request to the Heroku MIA host, which then facilitates the secure execution of the tool via the relevant MCP server. The result is returned to the agent, allowing it to incorporate that information into its response or subsequent actions.\n",
    "\n",
    "This abstraction allows you to focus on what you want your agent to do, rather than the intricate details of how it communicates with your backend systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Section 5: Q&A and Next Steps ---\n",
    "## Key Takeaways\n",
    "+ Heroku MIA simplifies the deployment and management of AI models.\n",
    "+ Heroku Tools empower AI agents to perform real-world actions within your applications.\n",
    "+ Model Context Protocol (MCP) provides the standardized backbone for secure and scalable agent interactions.\n",
    "\n",
    "## Resources\n",
    "+ Heroku Managed Inference and Agents: [https://www.heroku.com/ai/managed-inference-and-agents/](https://www.heroku.com/ai/managed-inference-and-agents/)\n",
    "+ Jupyter Notebooks on Heroku: [https://www.heroku.com/blog/jupyter-notebooks-heroku-persistent-storage/](https://www.heroku.com/blog/jupyter-notebooks-heroku-persistent-storage/)\n",
    "+ Using Heroku Tools with MIA: [https://devcenter.heroku.com/articles/heroku-inference-tools](https://devcenter.heroku.com/articles/heroku-inference-tools)\n",
    "+ Model Context Protocol (MCP) Explanation: [https://stytch.com/blog/model-context-protocol-introduction/](https://stytch.com/blog/model-context-protocol-introduction/)\n",
    "+ This Workshop Notebook: [Link to your GitHub repository (once created)]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
